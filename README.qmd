---
title: "Denoising Diffusion Models"
format: gfm
editor: visual
bibliography: references.bib
---

This repository contains a torch/luz implementation of the Denoising Diffusion Implicit Models. Code in this repository is heavily influenced by code in @kerasDDIM which is mostly based on [@song2020] with a few ideas coming from [@nichol2021] and [@karras2022].

Denoising Diffusion models are inspired by non-equilibrium thermodynamics [@sohl-dickstein2015]. First a forward diffusion algorithm is defined, this procedure converts any complex data distribution into a simple tractable distribution. We then learn a procedure to reverse the diffusion process.

While there's a strong theory foundation for denoising diffusion models, in practice, the core component is a neural network capable of separating a noisy image in its image and noise parts. For sampling new images we can then take pure noise and successively 'denoise' it until it's just the image part.

## Forward diffusion

Originaly the forward diffusion process has been defined as a Markov process that successively (for eg. for $T$ time steps) adds Gaussian noise[^1] to the data distribution until the resulting distribution is a standard Gaussian distribution. If we label the data distribution as $q(x_{0})$ we have the forward process defined as:

[^1]: [@bansal2022] seems to show that any lossy image transformation works

$$q(x_{t} | x_{t-1}) = \mathcal{N}(x_{t-1} \sqrt{1 - \beta_t}, I\beta_t )$$ where $\beta_t$ is the diffusion rate and $\beta_t \in (0,1)$. $\beta_t$ could be learned as in @sohl-dickstein2015, but usually a pre-defined schedule is used.

One important property of this process is that you can easily sample from $q(x_t | x_0)$. Using a reparametrization trick derived in [@ho2020] (see also [@weng2021diffusion]) one can express:

$$q(x_t | x_0) = \mathcal{N}(\sqrt{\bar{\alpha_t}}x_0, \sqrt{1-\bar{\alpha_t}}I)$$ And thus, $x_t$ can be expressed as a linear combination of $x_0$ and a Gaussian noise variable $\epsilon = \mathcal{N}(0, I)$:

$$x_t = \sqrt{\bar{\alpha_t}}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon$$

### Setting the diffusion rate

The diffusion rate $\bar{\alpha_t}$ schedule is in general a decreasing sequence of values, such that when $t$ is large, $x_t$ is almost pure noise. In our implementation the schedule is defined in terms of a continuous time variable so that we can change the number of diffusion steps as much as needed during sampling. $\bar{\alpha_t}$ is interpreted as the proportion of variance that comes from the original image ($x_0$) in $x_t$.

In @song2020 and @ho2020, a so called linear schedule is used. However the linearity is happening on $\beta_t$ - before the reparametrization. Thus, $\bar{\alpha_t}$ under the linear schedule doesn't vary linearly. @nichol2021 proposes that a cosine schedule can lead to better model performance, the cosine schedule is applied directly in $\bar{\alpha_t}$.

Below we can visualize the forward diffusion process with both the linear and cosine scheduling for 10 diffusion steps.

```{r schedules, echo=FALSE, out.width="704px", out.height="128px"}
#| fig-cap: Samples from $q(x_t|x_0)$ for linearly spaced values of $t$ with linear schedule (top) and the cosine schedule (bottom). We can see that with the linear schedule, images become almost pure noise after the first half - this seems to interfere in model performance according to @nichol2021 .
box::use(torch[...])
box::use(./diffusion[diffusion_schedule])
box::use(./dataset[make_dataset])
box::use(zeallot[...])
box::use(./callbacks[plot_tensors])

torch_manual_seed(1)
set.seed(1)

fitted <- luz::luz_load("luz_model.luz")
fitted$model$eval()
c(train_data, valid_data) %<-% make_dataset("flowers", c(64, 64))

img <- train_data[1]$x$view(c(1,3,64,64))

diffusion_times <- torch_tensor(seq(0, 1, length.out = 11))$view(c(-1, 1, 1, 1))
img <- fitted$model$normalize(img)
noise <- torch_randn(11, 3, 64, 64)

noisy_images <- lapply(c("linear", "cosine"), function(schedule_type) {
  schedule <- diffusion_schedule(schedule_type, 0, 1)
  rates <- schedule(diffusion_times)
  rates$signal*img + rates$noise*noise
})
noisy_images <- torch_cat(noisy_images, dim = 1)
noisy_images <- fitted$model$normalize$denormalize(noisy_images)

plot_tensors(noisy_images, ncol = 11)
```

## Reverse diffusion

The forward diffusion process thus is a fixed procedure to generate samples of $q(x_t|x_0)$, we are interested though in generating samples from $q(x_{t-1} | x_t)$ - known as reverse diffusion. Successively sampling this way would allow us to sample from $q(x_0 | x_T)$, ie, sample from the data distribution using only Gaussian noise.

As $q(x_{t-1} | x_t)$ is an unknown distribution, we will train a neural network $p_{\theta}(x_{t-1} | x_t, \bar{\alpha_t})$ to approximate it. We will skip the math details here, but a good step by step on the math can be found in [@weng2021diffusion].

The intuition though is that, we have $x_t$ and we know that it can be expressed as:

$$x_t = \sqrt{\bar{\alpha_t}}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon$$

Thus, if we can estimate $x_0$ from $x_t$ , since we know $\bar{\alpha_t}$ for every $t$ , we can then sample from $x_{t-1}$ by reusing the above equation. With this simplified view, our goal then is to train a neural networks that takes $x_t$ as input and returns an estimate $\hat{x}_0$ for $x_0$. Since we know how to sample from $q(x_t|x_0)$ we can generate as much training samples as needed. We can train this neural network to minimize $||x_0 - \hat{x}_0||^2$ or any other norm.

There's another way to get an estimate $\hat{x}_0$ though. If we get an estimate $\hat{\epsilon}$, we can replace it in the above equation and get an estimate for $x_0$. Empirically, this has been found by [@ho2020] to lead into better samples and training stability.

```{r reverse, out.width="704px", out.height="128px", echo=FALSE}
#| fig-cap: | 
#|   Forward diffusion (top) and reverse diffusion (bottom) using 10 difusion steps.
#|   The resulting noise from the forward diffusion process was used as input noise
#|   for the reverse diffusion process.
fitted <- luz::luz_load("luz_model.luz")
fitted$model$eval()
c(train_data, valid_data) %<-% make_dataset("flowers", c(64, 64))

img <- train_data[1]$x$view(c(1,3,64,64))

diffusion_steps <- 10

diffusion_times <- torch_tensor(seq(0, 1, length.out = diffusion_steps+1))$view(c(-1, 1, 1, 1))
img <- fitted$model$normalize(img)
noise <- torch_randn(diffusion_steps+1, 3, 64, 64)

schedule <- diffusion_schedule("cosine")
rates <- schedule(diffusion_times)
noisy_images <- rates$signal*img + rates$noise*noise

reverse_images <- torch_empty_like(noisy_images)
reverse_images[diffusion_steps+1,..] <- noisy_images[diffusion_steps+1, ..]
for (i in diffusion_steps:1) {
  r <- list(signal = rates$signal[i+1, .., drop=FALSE], noise =  rates$noise[i+1, .., drop=FALSE])
  c(p_noise, p_img) %<-% fitted$model$denoise(reverse_images[i+1, .., drop=FALSE], r)
  
  r <- list(signal = rates$signal[i, .., drop=FALSE], noise =  rates$noise[i, .., drop=FALSE])
  reverse_images[i,..] <- r$signal*p_img + r$noise*p_noise
}

reverse_images <- torch_flip(reverse_images, 1)

noisy_images <- torch_cat(list(noisy_images, reverse_images), dim = 1)
noisy_images <- fitted$model$normalize$denormalize(noisy_images)

plot_tensors(noisy_images, ncol = diffusion_steps + 1)
```


## Sinusoidal embedding

A sinusoidal embedding is used to encode the diffusion times into the model. The visualization below shows how diffusion times are mapped to the embedding - assuming the dimension size of 32. Each row is a embedding vector given the diffusion time. Sinusoidal embedding have nice properties, like preserving the relative distances [@kazemnejad2019:pencoding].

```{r sinusoidal, echo=FALSE}
box::use(torch[...])
box::use(./diffusion[sinusoidal_embedding, diffusion_schedule])

schedule <- diffusion_schedule()
embedding <- sinusoidal_embedding()

diffusion_times <- torch_linspace(0, 1, 100)$view(c(100, 1, 1, 1))
noise_power <- schedule(diffusion_times)$noise^2
embeddings <- embedding(noise_power)$add(1)$div(2)$squeeze(3)$squeeze(3)
heatmap(as.array(embeddings), Rowv=NA, Colv=NA, col = colorspace::diverge_hsv(50), scale = "none", labRow = sprintf("%.2f", seq(0,1,length.out = 100)))
```

## Sampling images

Images can be sampled from the model using the `generate` method. Remember to always set the model into `eval()` mode before sampling, so the batch normal layers are correctly applied.

```{r samples}
box::use(torch[...])
box::use(./callbacks[plot_tensors])

fitted <- luz::luz_load("luz_model.luz")

with_no_grad({
  fitted$model$eval()
  x <- fitted$model$generate(36, diffusion_steps = 5)$to(device = "mps")
})

plot_tensors(x)
```

## References
