---
title: "Denoising Diffusion Models"
format: gfm
editor: visual
bibliography: references.bib
---

This repository contains a torch/luz implementation of the Denoising Diffusion Implicit Models. Code in this repository is heavily influenced by code in @kerasDDIM which is mostly based on [@song2020] with a few ideas coming from [@nichol2021] and [@karras2022].

Denoising Diffusion models are inspired by non-equilibrium thermodynamics [@sohl-dickstein2015]. First a forward diffusion algorithm is defined, this procedure converts any complex data distribution into a simple tractable distribution. We then learn a procedure to reverse the diffusion process.

While there's a strong theory foundation for denoising diffusion models, in practice, the core component is a neural network capable of separating a noisy image in its image and noise parts. For sampling new images we can then take pure noise and successively 'denoise' it until it's just the image part.

## Forward diffusion

Originaly the forward diffusion process has been defined as a Markov process that successively (for eg. for $T$ time steps) adds Gaussian noise to the data distribution
until the resulting distribution is a standard Gaussian distribution. Ie, labelling the data distribution as $q(x_{0})$ we have the forward process defined as:

$$q(x_{t} | x_{t-1}) = \mathcal{N}(x_{t-1} \sqrt{1 - \beta_t}, I\beta_t )$$
where $\beta_t$ is the diffusion rate and $\beta_t \in (0,1)$. $\beta_t$ could be learned as in @sohl-dickstein2015, but it's usually, a pre-defined schedule is used. 

One important property of this process is that you can easily sample from $q(x_t | x_0)$. Using a reparametrization trick derived in @nichol2021 (see also [@weng2021diffusion]) one can express:

$$q(x_t | x_0) = \mathcal{N}(\sqrt{\alpha_t}x_0, \sqrt{1-\alpha_t}I)$$
And thus, $x_t$ can be expressed as a linear combination of $x_0$ and a gaussian noise variable $\epsilon = \mathcal{N}(0, I)$:

$$x_t = \sqrt{\alpha_t}x_0 + \sqrt{1-\alpha_t}\epsilon$$

## Sinusoidal embedding

A sinusoidal embedding is used to encode the diffusion times into the model. The visualization below shows how diffusion times are mapped to the embedding - assuming the dimension size of 32. Each row is a embedding vector given the diffusion time. Sinusoidal embedding have nice properties, like preserving the relative distances [@kazemnejad2019:pencoding].

```{r sinusoidal, echo=FALSE}
box::use(torch[...])
box::use(./diffusion[sinusoidal_embedding, diffusion_schedule])

schedule <- diffusion_schedule()
embedding <- sinusoidal_embedding()

diffusion_times <- torch_linspace(0, 1, 100)$view(c(100, 1, 1, 1))
noise_power <- schedule(diffusion_times)$noise^2
embeddings <- embedding(noise_power)$add(1)$div(2)$squeeze(3)$squeeze(3)
heatmap(as.array(embeddings), Rowv=NA, Colv=NA, col = colorspace::diverge_hsv(50), scale = "none", labRow = sprintf("%.2f", seq(0,1,length.out = 100)))
```

## Sampling images

Images can be sampled from the model using the `generate` method. Remember to always set the model into `eval()` mode before sampling, so the batch normal layers are correctly applied.

```{r samples}
box::use(torch[...])
box::use(./callbacks[plot_tensors])

fitted <- luz::luz_load("luz_model.luz")

with_no_grad({
  fitted$model$eval()
  x <- fitted$model$generate(36, diffusion_steps = 5)$to(device = "mps")
})

plot_tensors(x)
```

## References
